# Machine-Learning-Course-Theory-Fundamental.
## Definition of Machine-Learning:
Machine Learning is a branch of Artificial Intelligence and Computer Science and Engineering.
Machine Learing is a field of study that enables the system to learn from experience by using data.


## Goals of Machine Learning:
i) enable machine that can learn from data, make prediction and solve tasking without explicit programming.
ii)enhancing automation and decision making across various domains through pattern recognition and data-driven model.

## Why Machine learning getting so mush attention recently:

1. The amount of data generation is increasing significantly and reduce the cost of sensors.  
1. The cost of storing this data has reduced significantly.  
1. The cost of computing has come down significantly.  
1. Cloud has democratized compute for the masses.  
1. Notable successes in areas like:  
        - image and speech recognition  
        - natural language processing  
        - recommendation systems  
        - self-driving cars.  

These success stories demonstrate the real-world applications and potential of machine learning, driving further interest and investment in the field.  

## Workflow/Steps of Machine Learning:
![image](https://github.com/Azizur-Rahman-CSE/Machine-Learning-Course-Theory-Fundamental./assets/57495952/51ce4ceb-db61-4768-ae12-af87898c3307)



## Lecture 2-3
Supervised Learning is also called Predictive Modeling. Supervised Learning is 2 types:
1. Regression: Real/Continious value predition Model. example: House price prediction.
1. Classification: Predict the class of the model. example: apple and banana class prediction, Language detection etc.

![image](https://github.com/Azizur-Rahman-CSE/Machine-Learning-Course-Theory-Fundamental./assets/57495952/ad96289f-125c-485a-bb1c-11441c59e5cd)

## classification:
1. Binary Classification: Classifying instances into one of two classes/categories, for example, email spam filtering (spam or not).  
1. Multiclass Classification: Classifying instances into one of three or more classes/categories, for example, color of an object, it may be red or green or blue.  
1. Multi-Label Classification: Multiple labels/classes are to be predicted for each instance, for example, when predicting a given movie category, it may belong to horror, romance, adventure, action, or all simultaneously.

## Some Classification Problems
1. Spam filtering
2. Language detection
3. Document/text classification
4. Sentiment analysis of text (positive, negative, or neutral)
5. Recognition of handwritten characters and numbers
6. Fraud detection
7. Disease prediction and detection etc.

## Regression:
Regression in supervised machine learning is a statistical method used for modeling the relationship between a dependent variable (target) and one or more independent variables (predictors). Its primary goal is to predict the value of the dependent variable based on the values of the independent variables.

### When to use regression:
<ins> Prediction:</ins> Regression is used when you want to predict a continuous output variable. For instance, predicting house prices based on features like size, location, and number of bedrooms.  

<ins>Forecasting:</ins> Regression is valuable in forecasting future trends or outcomes based on historical data patterns.  

<ins>Risk Assessment:</ins> In fields like finance and insurance, regression can be used to assess risks by predicting potential losses or gains based on various factors.  

<ins>Optimization:</ins> Regression models can also be utilized in optimization problems, where the goal is to find the best possible outcome.  

## List of Commonly Supervised Learning Algorithms  
1. Linear Regression
2. Logistic Regression
3. k-Nearest Neighbors (kNN)
4. Decision Trees
5. Random Forest
6. Support Vector Machines (SVM)
7. Gradient Boosting Machines(GBM)
8. LightGBM
9. XGBoost
10. CatBoost
11. Neural Networks

## Unsupervised learning

Unsupervised learning is a type of machine learning where the algorithm learns patterns, relationships, and structures from input data without explicit supervision or labeled responses.  
The primary goal of unsupervised learning is to explore and identify hidden patterns, groupings, or structures within the data.  

## Unsupervised Machine Learning Types:

1. <ins>Clustering</ins>: Clustering is an unsupervised learning technique that groups similar data points together based on their shared characteristics or features, without predefined labels.
![image](https://github.com/Azizur-Rahman-CSE/Machine-Learning-Course-Theory-Fundamental./assets/57495952/0ffaba55-8ba2-4f54-b8b4-c28f993a4544)
### application of clustering:
        - Customer Segmentation in Marketing  
        - Image Segmentation in Computer Vision  
        - Anomaly Detection: Identifying outliers or anomalies in datasets, such as detecting fraudulent activities in finance.  
        - Social Network Analysis  
        - Traffic Flow Analysis  
3. <ins>Association Rule Mining</ins>:  
        -  Association Rule Mining is a rule-based machine learning.  
        -  method for discovering frequently occurring patterns, correlations, or associations between variables in large databases.  
           -  It is intended to identify strong rules discovered in databases  
![image](https://github.com/Azizur-Rahman-CSE/Machine-Learning-Course-Theory-Fundamental./assets/57495952/037ef0c5-7a78-4b39-9b5b-145aef2b6322)

![image](https://github.com/Azizur-Rahman-CSE/Machine-Learning-Course-Theory-Fundamental./assets/57495952/6a5e77cb-f2df-40b8-9244-7778a731e72c)

## Semisupervised learning
1. Semisupervised learning can deal with partially labeled training data,
2. usually a lot of unlabeled data and a little bit of labeled data.
3. Most semisupervised learning algorithms are combinations of
unsupervised and supervised algorithms.  
Example: Some photo-hosting services, such as Google Photos. Once you
upload all your family photos to the service, it automatically recognizes
that the same person A shows up in photos 1, 5, and 11, while another
person B shows up in photos 2, 5, and 7 (unsupervised learning). Now all
the system needs is for you to tell it who these people are (Supervised
learning). Just one label per person, and it is able to name everyone in
every photo, which is useful for searching photos.

## Reinforcement Learning
1. Reinforcement learning is a feedback-based learning method, in which a
learning agent gets a reward for each right action and gets a penalty for each
wrong action.
2. The agent learns automatically with these feedbacks and improves its
performance.


![image](https://github.com/Azizur-Rahman-CSE/Machine-Learning-Course-Theory-Fundamental./assets/57495952/264fc58f-b981-4cc5-810a-8cbc8de0840c)

![image](https://github.com/Azizur-Rahman-CSE/Machine-Learning-Course-Theory-Fundamental./assets/57495952/df1d5b05-9cdb-4a36-9052-f7c04e459e50)


## Machine Learning Lession-Data
1. Data can be any unprocessed fact, value, text, sound, picture or video that is not being interpreted and analyzed.
2. Data is the most important part of all Data Mining, Machine Learning, Artificial Intelligence
3. Without data, we can’t train any model and all modern research and automation will go vain
4. Big Enterprises are spending loads of money just to gather as much certain data as possible  
Example: Facebook acquires WhatsApp by paying a huge price of $19
billion

### Information and Knowledge  
Information: Processed, organized, or structured data to provide context
and meaning.  
Knowledge: Combination of inferred information, experiences, learning
and insights. Knowledge is useful and actionable information that can
lead to impact.

![image](https://github.com/Azizur-Rahman-CSE/Machine-Learning-Course-Theory-Fundamental./assets/57495952/3f0da3e8-5792-466e-8f37-87fcc5a37721)

![image](https://github.com/Azizur-Rahman-CSE/Machine-Learning-Course-Theory-Fundamental./assets/57495952/4fb8c8f1-30d5-4a2d-a0eb-3f9453b0e665)



## Lecture-5
### Data quality:
Data quality refers to how reliable, accurate, complete, and relevant data is for its intended purpose or use.  
Components of data quality:
1. Accuracy
2. Completeness
3. Consistency
4. Conformity
5. Timeliness
6. Intigrity.
### Accuracy:
Data is considered accurate if it correctly represents the real-world values or facts it intends to describe without errors or inconsistencies.  
For example, in a sales database, if a customer's purchase amount is recorded as $500, but in reality, it was $550, there is an inaccuracy of $50. This discrepancy between the recorded data and the actual value demonstrates a lack of accuracy in the dataset.  

### Completeness:
 Complete data contains all the necessary information required for its intended purpose.  
 Data can be complete even if optional data is missing. As long as the data meets the expectations then the data is considered complete.  
 For example, a customer’s first name and last name are mandatory but middle name is optional; so a record can be considered complete even if a middle name is not available.  

 ### Consistency:
 Consistent data is consistence across different sources or within a dataset. It should not have contradictions or duplications.  
 Examples of some inconsistencies:  
 1. A business unit status is closed but there are sales for that business unit.
 2. Employee status is terminated but pay status is active.  

### Conformity:
Conformity means the data is following the set of standard data definitions  
example: date of birth of customer is in the format “mm/dd/yyyy”.

### Timeliness:
Timely data is up-to-date and relevant for the purpose it serves. Outdated information may lose its value or relevance over time.  

### Intigrity:
Integrity means validity of data across the relationships.  
For example, in a customer database, there should be a valid customer, address and relationship between them.  

## Data Quality Problems 
◦ Noise  
◦ Outliers  
◦ Missing values  
◦ Duplicate or Redundant data  
### Noise
In the context of data analysis, "data noise" refers to irrelevant, random, or extraneous information within a dataset.  
1. Noisy data (or corrupt data) are meaningless information
2. It cannot be understood and interpreted correctly by machin
3. It unnecessarily increases the amount of storage space required
Noisy data can be caused by faulty data collection instruments, human or computer errors.

### How to Handle Noisy Data
1. Remove noise from data (called data smoothing) using binning method, regression, clustering.
2. Collect more data, it’s the best way to cut the noise out but data is expensive.
3. Use Principal Component Analysis (PCA) for dimensionality reduction.
4. Use regularization and cross validation (CV) to prevent overfitting.

### Outliers:
Outliers are data objects with characteristics that are considerably different than most of the other data objects in the data set.

![image](https://github.com/Azizur-Rahman-CSE/Machine-Learning-Course-Theory-Fundamental./assets/57495952/c84580e4-bf91-4ccb-84d8-452ad1847ca5)


### How to Handle Outliers:
1. Drop the outlier records.
2. Assign a new value: If an outlier seems to be due to a mistake in your data, try imputing a new value.
3. Try a transformation.
4. Identification and analysis:  
Visualizations like box plots, scatter plots, or histograms can help identify outliers visually.

### Missing Values:
Reasons for missing values
◦ Information is not collected  
(e.g., people decline to give their age and weight)
◦ Attributes may not be applicable to all cases
(e.g., annual income is not applicable to children)  


#### Handling missing values
◦ Eliminate Data Objects
◦ Estimate Missing Values (Mean/ Mode/ Median /Prediction etc.)
◦ Ignore the Missing Value During Analysis
◦ Replace with all possible values (weighted by their probabilities)

#### Dealing with duplicate data
1. You should probably remove duplicate data.
2. Duplicate data will essentially lead to bias your fitted model or do the model overfitting.


# Data Preprocessing

#### definition:
Data preprocessing is the cleaning and organizing of raw data before using it for analysis or machine learning. It involves fixing errors, handling missing information, converting data into a usable format.  

### Data Preprocessing Techniques:
1. Data cleaning (fix noises, outliers, missing values, duplicates in data)
2. Aggregation
3. Sampling
4. Dimensionality reduction
5. Feature subset selection
6. Feature creation

### Aggregation:
Combining two or more attributes (or objects) into a single attribute (or object).  

#### Purpose
◦ Data reduction  
        - Reduce the number of attributes or objects
◦ Change of scale  
        - Cities aggregated into regions, states, countries, etc.
◦ Less memory, less processing time

#### Disadvantage: the potential loss of interesting details.

## Sampling:
Sampling refers to the process of selecting a subset of items, or data points from a larger population or dataset in order to gather insights.

![image](https://github.com/Azizur-Rahman-CSE/Machine-Learning-Course-Theory-Fundamental./assets/57495952/b63910f8-0da4-4e24-bad6-9819c2cfa17f)

#### Representative Sample:
The key principle for effective sampling is the following:  
◦ Using a sample will work almost as well as using the entire data sets (or
population), if the sample is representative.  
◦ A sample is representative if it has approximately the same property (of
interest) as the original set of data 

![image](https://github.com/Azizur-Rahman-CSE/Machine-Learning-Course-Theory-Fundamental./assets/57495952/0d1dbc82-fc0f-48e3-ad08-381853ff3e5a)

#### Types of Sampling Methods
1. Simple Random Sampling
2. Systematic Sampling
3. Stratified Sampling
4. Cluster Sampling
5. Multistage sampling

![image](https://github.com/Azizur-Rahman-CSE/Machine-Learning-Course-Theory-Fundamental./assets/57495952/0f450681-83b7-4f02-bbb0-f16b97090209)

![image](https://github.com/Azizur-Rahman-CSE/Machine-Learning-Course-Theory-Fundamental./assets/57495952/d2c631b3-5e9d-45d1-bc1a-b1b733c40194)

![image](https://github.com/Azizur-Rahman-CSE/Machine-Learning-Course-Theory-Fundamental./assets/57495952/cc0cd3d5-72a2-4935-8603-0bf500da278f)

![image](https://github.com/Azizur-Rahman-CSE/Machine-Learning-Course-Theory-Fundamental./assets/57495952/f022d976-8387-4da7-b879-dfa39862329f)

![image](https://github.com/Azizur-Rahman-CSE/Machine-Learning-Course-Theory-Fundamental./assets/57495952/846e02a3-3df3-4223-b755-37e7f4c2f8e2)

### Progressive sampling:  
Start with a small sample, and then increase the size until a sufficient sample has been obtained.

## Dimensionality Reduction
#### Purpose:
◦ Avoid curse of dimensionality
◦ May help to eliminate irrelevant features or reduce noise
◦ Reduce amount of time and memory required by data mining algorithms
◦ Allow data to be more easily visualized
◦ Allow model to be more understandable
#### Techniques:
◦ Principle Component Analysis (PCA)
◦ Singular Value Decomposition (SVD)
◦ Others: supervised and non-linear techniques

### Dimensionality Reduction vs Feature Subset Selection:
#### Dimentionality reduction:
1. Reduce features by creating fewer but representative ones.  
2. Transforms original features into a smaller set.
3. Keeping crucial information while reducing complexity.
4. computationally Complex

5. #### Feature Subset Selection:
   1. Pick the most important features without changing them.
   2. Selects the best features from the existing ones.
   3. Choosing the most informative features.
   4. Usually less demanding computationally.

### Feature Selection:
Feature selection is the process of choosing the most important features from a dataset to make machine learning models more accurate and efficient. deleting irrelevent feature.  
Here are some techniques used for feature selection:  
1. <ins>Filter Methods:</ins> Select features based on statistical measures or correlations with the target variable.
2. <ins>Wrapper Methods:</ins> Evaluate different feature subsets using the model's performance to choose the best set.
3. <ins>Embedded Selection:</ins>  
           1. It uses a supervised machine learning model to judge the importance ofeach feature, and keeps only the most important ones.  
           2. More important features are assigned a higher weight, while less important
features are given a lower weight.
           3. Some machine learning algorithms (e.g. Decision trees, SVM, GBM) perform
automatic feature selection during model training.Like: LASSO Algo.

